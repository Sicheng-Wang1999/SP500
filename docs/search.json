[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "SP500",
    "section": "",
    "text": "1 Introduction\nThis project aims to unravel the mysteries and trends hidden behind the S&P 500 Index. The S&P 500, or Standard & Poor’s 500, represents a collection of 500 leading companies and takes up approximately 80% of available market capitalization. Taking Exploratory Data Analysis as our compass, we adventure through the vast financial landscape of historical returns, risks, and sectors within the index."
  },
  {
    "objectID": "index.html#why-explore-sp-500",
    "href": "index.html#why-explore-sp-500",
    "title": "SP500",
    "section": "1.1 Why Explore S&P 500",
    "text": "1.1 Why Explore S&P 500\nThe S&P 500 gives us a snapshot of how well the economy is doing. If just a few companies is not doing well on a single day, it doesn’t drag the index down as there are 500 companies. Thus, S&P 500 often serves as a benchmark for US stock markets, that people often compare with their portfolio with to assess the standing of performance and diversity of individual portfolio relative to the overall market. By exploring the SP500 index on its correlation with economic factors such as interest rates, and through breakdowns by location of headquarter or sector of individual stock. We hope to find hints on how to build the right portfolio at the right time."
  },
  {
    "objectID": "index.html#what-to-expect",
    "href": "index.html#what-to-expect",
    "title": "SP500",
    "section": "1.2 What to Expect",
    "text": "1.2 What to Expect\n\n1. Performance of Index v.s. individual stock\nWe are interested in seeing the trends (ups and downs, seasonality, significant milestones) of the S&P 500 Index within 5 years and its comparison with individual constituents of the index. Which subset of the pool outperforms the index.\n\n\n2. Sector/location v.s. Performance\nAfter locating the companies that have over/under index performance, we would like to investigate if that has any correlation with certain properties of the company including headquarter location and business sector.\n\n\n3. Resilience v.s. Performance\nIdentify bad times, and see how soon the economy recovers. Is it true that companies that can recover fast from bad economic situations will succeed in the future?\n\n\n4. Interest Rate/Unemployment Rate v.s. S&P 500 index\nHow are macroeconomic factors including Interest Rate/Unemployment Rate related to the S&P 500 index? Does low interest rate imply steady growth?"
  },
  {
    "objectID": "data.html#description",
    "href": "data.html#description",
    "title": "2  Data",
    "section": "2.1 Description",
    "text": "2.1 Description\n\nInterest rate data from 2022-2023 Daily interest rate data is collected by U.S Department of the Treasury and is downloaded as csv files. The data consists of 1 month rates to 30 year rates and is updated daily, note that since 10/18/2022 there are no 4 months interest rates.\n\n\n\nCode\nAPR22 = read.csv(\"daily-treasury-rates-2022.csv\")\nAPR23 = read.csv(\"daily-treasury-rates-2023.csv\")\nAPR = bind_rows(APR22,APR23)\nhead(APR)\n\n\n        Date X1.Mo X2.Mo X3.Mo X4.Mo X6.Mo X1.Yr X2.Yr X3.Yr X5.Yr X7.Yr X10.Yr\n1 12/30/2022  4.12  4.41  4.42  4.69  4.76  4.73  4.41  4.22  3.99  3.96   3.88\n2 12/29/2022  4.04  4.39  4.45  4.66  4.73  4.71  4.34  4.16  3.94  3.91   3.83\n3 12/28/2022  3.86  4.33  4.46  4.66  4.75  4.71  4.31  4.18  3.97  3.97   3.88\n4 12/27/2022  3.87  4.32  4.46  4.66  4.76  4.75  4.32  4.17  3.94  3.93   3.84\n5 12/23/2022  3.80  4.20  4.34  4.59  4.67  4.66  4.31  4.09  3.86  3.83   3.75\n6 12/22/2022  3.80  4.20  4.35  4.57  4.66  4.64  4.24  4.02  3.79  3.77   3.67\n  X20.Yr X30.Yr\n1   4.14   3.97\n2   4.09   3.92\n3   4.13   3.98\n4   4.10   3.93\n5   3.99   3.82\n6   3.91   3.73\n\n\nDownloaded sp500 portfolio from https://www.slickcharts.com/sp500 as sp500rank.xlsx\n\n\nCode\nsp500rank &lt;- read_excel(\"sp500rank.xlsx\")\nsp500rank_list = unlist(sp500rank['Symbol'])\ntop_105_symbols = sp500rank_list[1:105]\nlast_105_symbols = rev(rev(sp500rank_list)[1:105])\n\n\nThe stock symbols are then used to download 20 year daily movement from yahoo finance\n\n\nCode\noptions(\"getSymbols.warning4.0\"=FALSE)\noptions(\"getSymbols.yahoo.warning\"=FALSE)\n#getSymbols(top_105_symbols, from = '2003-10-30', to = \"2023-10-30\",warnings = FALSE)\n#for (symbol in top_105_symbols) {\n#  if (exists(symbol)) {\n#    write.zoo(get(symbol), glue('data/{symbol}.csv'), sep = \",\")\n#  }\n#}\n#getSymbols(last_105_symbols, from = '2003-10-30', to = \"2023-10-30\",warnings = FALSE)\n#for (symbol in last_105_symbols) {\n#  if (exists(symbol)) {\n#    write.zoo(get(symbol), glue('data/{symbol}.csv'), sep = \",\")\n#  }\n#}\n\n\n\n\nCode\ngetSymbols('^GSPC', from = '2003-10-30', to = \"2023-10-30\",warnings = FALSE)\n\n\n[1] \"GSPC\"\n\n\nCode\nwrite.zoo(GSPC, 'data/GSPC.csv', sep = \",\")\n\n\nData collected:\n\n2022-2023 Daily interest rate data from US Treasury, by Sicheng.\nUp-to-date S&P500 portfolio data from https://www.slickcharts.com/sp500. The data is collected simply by copying the elements in the html table and paste into an excel sheet in xlsx. It has the rank, name, symbol, for each stock in the S&P500. This data helps us identify the stocks in S&P500 and how important they are by looking at the percentage each stock takes in the portfolio. The data is ranked by the current percentage and this helps us identify the important stocks and less important stocks in S&P500, allowing us to further explore individual stocks, by Shuai. A note is that there are more than 500 entries in S&P500 as some companies have more than one stock. For example, GOOGL and GOOG are both Google’s.\n20 years daily movement data from 2003-10-30 or as early as possible to 2023-10-27, for each stocks ranked in top 105 and each stocks ranked in last 105. The data is collected using tidyquant package and the source of data is Yahoo Finance according to the package. They are then written into separate csv files store under the data directory. For each stock, the csv file contains the date, open, close, highest, and lowest price, as well as the trade volume and adjusted price. Collecting these stock data allows us to explore the individual stocks performance and explore the differences in performance between the most important stocks and the least important stocks, as well as how the stocks are different. For example, are the less important stocks smaller and do they have smaller trade volume? or do they perform worse or not? by Shuai. A note is that the codes are commented out because downloading the data from Yahoo takes an extensive amount of time. Local data is csv will be available to use.\n20 years daily movement data from 2003-10-30 to 2023-10-27 for S&P500. It is sourced, formatted, and stored the same way as the individual stock data. Using this data, we can compare the individual stock performance with the S&P500 index and explore. For example, is there any stock that actually moves against S&P500, or are there any outliers? by Shuai."
  },
  {
    "objectID": "data.html#missing-value-analysis",
    "href": "data.html#missing-value-analysis",
    "title": "2  Data",
    "section": "2.2 Missing value analysis",
    "text": "2.2 Missing value analysis\n\n\nCode\ndf &lt;- APR |&gt; column_to_rownames(\"Date\")\ndf &lt;- df |&gt; rownames_to_column(\"id\") |&gt; pivot_longer(cols = -id) |&gt; mutate(missing = ifelse(is.na(value),\"yes\",\"no\"))\ndf &lt;- df |&gt; mutate(id = as.Date(id, format = \"%m/%d/%Y\"))\nggplot(df, aes(x=name, y=id, fill = missing)) + \n    geom_tile()\n\n\n\n\n\nBefore 10/18/2022 there are no 4 months interest rates.\n\n\nCode\nstart_date &lt;- as.Date(\"2003-10-29\")\nend_date &lt;- as.Date(\"2023-11-05\")\ndate_sequence &lt;- seq(start_date, end_date, by = \"1 day\")\n\ndf &lt;- data.frame(Date = date_sequence)\ndf$Week_Start_Date &lt;- floor_date(df$Date, unit = \"week\")\nunique_week_start_dates &lt;- unique(df$Week_Start_Date)\n\ntemplate_df &lt;- data.frame(\n  Sunday = rep(0, length(unique_week_start_dates)),\n  Monday = rep(0, length(unique_week_start_dates)),\n  Tuesday = rep(0, length(unique_week_start_dates)),\n  Wednesday = rep(0, length(unique_week_start_dates)),\n  Thursday = rep(0, length(unique_week_start_dates)),\n  Friday = rep(0, length(unique_week_start_dates)),\n  Saturday = rep(0, length(unique_week_start_dates))\n)\n\nresult_df &lt;- cbind.data.frame(Week_Start_Date = unique_week_start_dates, template_df)\n\nresult_df &lt;- result_df %&gt;%\n  select(Week_Start_Date, Sunday, Monday, Tuesday, Wednesday, Thursday, Friday, Saturday)\n\n\n\n\nCode\nsp500_missing &lt;- result_df\nsp500_dates &lt;- as.Date(unlist(read.csv(\"data/GSPC.csv\")['Index']))\nfor (date in sp500_dates) {\n    date &lt;- as.Date(date)\n    week_start &lt;- floor_date(date, unit = \"week\")\n    day_of_week &lt;- weekdays(date)\n    day_of_week &lt;- match(day_of_week, c(\"Sunday\", \"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\"))\n    sp500_missing[sp500_missing$Week_Start_Date == week_start, day_of_week] &lt;- 1\n}\n\n\n\n\nCode\nmeta_missing &lt;- result_df\nmeta_dates &lt;- as.Date(unlist(read.csv(\"data/META.csv\")['Index']))\nfor (date in meta_dates) {\n    date &lt;- as.Date(date)\n    week_start &lt;- floor_date(date, unit = \"week\")\n    day_of_week &lt;- weekdays(date)\n    day_of_week &lt;- match(day_of_week, c(\"Sunday\", \"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\"))\n    meta_missing[meta_missing$Week_Start_Date == week_start, day_of_week] &lt;- 1\n}\n\n\n\n\nCode\nlong_df &lt;- pivot_longer(sp500_missing, cols = -Week_Start_Date, names_to = \"Day\", values_to = \"Value\")\n\nggplot(long_df, aes(x = Week_Start_Date, y = Day, fill = factor(Value))) +\n  geom_tile() +\n  scale_fill_manual(values = c(\"1\" = \"cornflowerblue\", \"0\" = \"red\")) +\n  labs(fill = \"Value\", x = \"Week Start Date\", y = \"Day of the Week\", title = \"S&P500\") +\n  coord_flip()\n\n\n\n\n\n\n\nCode\nlong_df &lt;- pivot_longer(meta_missing, cols = -Week_Start_Date, names_to = \"Day\", values_to = \"Value\")\n\nggplot(long_df, aes(x = Week_Start_Date, y = Day, fill = factor(Value))) +\n  geom_tile() +\n  scale_fill_manual(values = c(\"1\" = \"cornflowerblue\", \"0\" = \"red\")) +\n  labs(fill = \"Value\", x = \"Week Start Date\", y = \"Day of the Week\", title = \"META\") +\n  coord_flip()\n\n\n\n\n\nThere is no missing value in the stock data. However, due to some stocks entering the market late, earlier data does not exist for those stocks.\nWhile there is no missing data, market closes during holidays, so some dates will miss, but the data from Yahoo is accurate to use."
  },
  {
    "objectID": "results.html#individual-stock-analysis",
    "href": "results.html#individual-stock-analysis",
    "title": "3  Results",
    "section": "3.1 Individual stock analysis",
    "text": "3.1 Individual stock analysis\nFirst we would like to show the percentage of investment in a cumulative curve.\n\n\nCode\nsp500rank$CumulativePercent &lt;- cumsum(sp500rank$\"Portfolio%\")\nggplot(sp500rank, aes(x = Rank, y = CumulativePercent)) +\n  geom_line() + # Line plot for the cumulative sum\n  theme_minimal() + # Minimal theme for the plot\n  labs(title = \"Cumulative Portfolio Percentage by Rank\",\n       x = \"Rank\",\n       y = \"Cumulative Portfolio\")\n\n\n\n\n\nThis graph shows that the first few stocks take over 25% of the portfolio, the first 50 stocks take over 50% of the portfolio, and the first 125 stocks take about 75% of the total portfolio.\n\n\nCode\nsp500rank$RankGroup &lt;- replace_na(cut(sp500rank$Rank, breaks=seq(0, max(sp500rank$Rank), by=101), include.lowest=TRUE, labels=FALSE), 5)\nggplot(sp500rank, aes(x = Rank, y = CumulativePercent)) +\n  geom_line() +\n  theme_minimal() +\n  facet_wrap(~RankGroup, scales = \"free\") +\n  labs(title = \"Cumulative Portfolio Percentage by Rank\",\n       x = \"Rank\",\n       y = \"Cumulative Portfolio\")\n\n\n\n\n\nDifferent from the overall graph, we can see that except for the first 101 stocks, the cumulative lines look more linear with only small mount of curve in the middle. This means that the movement of the top stocks should have significant impact on the movement of the S&P 500 index.\nWe will now look into the movement of the top stocks and bottom stocks.\n\n\nCode\ntop_25_symbols = sp500rank_list[1:25]\nlast_25_symbols = rev(rev(sp500rank_list)[1:25])\ntop_25_df = list()\nfor (stock in top_25_symbols) {\n  assign(stock, read.csv(glue('data/{stock}.csv')), envir = .GlobalEnv)\n  top_25_df[[stock]] &lt;- get(stock)\n}\ntop_25_df &lt;- lapply(top_25_df, function(df) {\n  names(df) &lt;- gsub(\"^[^.]*\\\\.\", \"\", names(df))\n  return(df)\n})\ntop_25_df &lt;- lapply(top_25_df, function(df) {\n  names(df) &lt;- gsub(\"^[^.]*\\\\.\", \"\", names(df))\n  return(df)\n})\nbot_25_df = list()\nfor (stock in last_25_symbols) {\n  assign(stock, read.csv(glue('data/{stock}.csv')), envir = .GlobalEnv)\n  bot_25_df[[stock]] &lt;- get(stock)\n}\nbot_25_df &lt;- lapply(bot_25_df, function(df) {\n  names(df) &lt;- gsub(\"^[^.]*\\\\.\", \"\", names(df))\n  return(df)\n})\ncombined_top25_df &lt;- bind_rows(top_25_df, .id = 'symbol')\ncombined_bot25_df &lt;- bind_rows(bot_25_df, .id = 'symbol')\ncombined_top25_df$Date &lt;- as.Date(combined_top25_df$Index)\ncombined_bot25_df$Date &lt;- as.Date(combined_bot25_df$Index)\n\n\nLet’s first take a look at the S&P500 movement overt the past 20 years.\n\n\nCode\nsp500 &lt;- read.csv('data/GSPC.csv')\nsp500$Date &lt;- as.Date(sp500$Index)\nggplot(sp500, aes(x = Date, y = GSPC.Close)) + \n  geom_line() + \n  theme_minimal() + \n  labs(title = \"Closing Price by Date\",\n       x = \"Date\",\n       y = \"Closing Price\") +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1))\n\n\n\n\n\nAs we can see, it shows a steady growth over the past 20 years.\nNow, let’s look at the top 25 stocks form the S&P500.\n\n\nCode\nggplot(combined_top25_df, aes(x = Date, y = Close, group = symbol)) + \n  geom_line() + \n  facet_wrap(~ symbol, scales = \"free_y\") + \n  theme_minimal() + \n  labs(title = \"Closing Price by Date\",\n       x = \"Date\",\n       y = \"Closing Price\") +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1))\n\n\n\n\n\nIn the plot, most of them show a similar pattern of increase from 2003 to 2023 as S&P500, except for a few stocks including Tesla and Nvidia that didn’t have obvious growth until recent, as well as Exxon Mobil (XOM) and Chevron(CVX) that fluctuated around a certain price for about 15 years without obvious growth pattern.\nThen, let’s take a look at the bottom 25 stocks from S&P500.\n\n\nCode\nggplot(combined_bot25_df, aes(x = Date, y = Close, group = symbol)) + \n  geom_line() + \n  facet_wrap(~ symbol, scales = \"free_y\") + \n  theme_minimal() + \n  labs(title = \"Closing Price by Date\",\n       x = \"Date\",\n       y = \"Closing Price\") +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1))\n\n\n\n\n\nThis looks more interesting than the last graph, because now we see many different patterns where almost none of them look close to the S&P500 movement. For example, Zions Bancorporation (ZION) started at its historical high price and then had a big price drop. Bio-Rad Laboratories(BIO), CapitaLand China Trust(CL), and Generac Holdings(GNRC) all show a similar pattern, all had a sudden peak between 2021-2022, and all soon dropped back to the price before peak. This raise our further interest to explore the next batch of stocks after the top 25 ones - will they look more like the s&p500 pattern? or will they look more like the last 25 stocks?\n\n\nCode\nnext_25_symbols = sp500rank_list[26:50]\nnext_25_df = list()\nfor (stock in next_25_symbols) {\n  assign(stock, read.csv(glue('data/{stock}.csv')), envir = .GlobalEnv)\n  next_25_df[[stock]] &lt;- get(stock)\n}\nnext_25_df &lt;- lapply(next_25_df, function(df) {\n  names(df) &lt;- gsub(\"^[^.]*\\\\.\", \"\", names(df))\n  return(df)\n})\nnext_25_df &lt;- lapply(next_25_df, function(df) {\n  names(df) &lt;- gsub(\"^[^.]*\\\\.\", \"\", names(df))\n  return(df)\n})\ncombined_next25_df &lt;- bind_rows(next_25_df, .id = 'symbol')\ncombined_next25_df$Date &lt;- as.Date(combined_next25_df$Index)\n\n\n\n\nCode\nggplot(combined_next25_df, aes(x = Date, y = Close, group = symbol)) + \n  geom_line() + \n  facet_wrap(~ symbol, scales = \"free_y\") + \n  theme_minimal() + \n  labs(title = \"Closing Price by Date\",\n       x = \"Date\",\n       y = \"Closing Price\") +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1))\n\n\n\n\n\nThe stocks in this plot show somewhat closer to our first batch as most of them have a steady growth over the past 20 years, which is closer to the S&P500 curve. However, we can observe that in both this plot and the bottom 25 stocks plot, there are many stocks that showed steady increase until recent years, and then kept going down until now, including ABT, ACN, AMD, CMCSA, CRM, and so on… Their drops in price are somewhat larger than the S&P500 index, which shows the resiliency of the S&P500 to price change.\nNext, after seeing the overall trend, we would like to explore the yearly price change of the individual stocks for recent years.\n\n\nCode\nyearly_prices &lt;- combined_top25_df %&gt;%\n  mutate(Year = year(Date)) %&gt;%\n  group_by(symbol, Year) %&gt;%\n  summarize(YearStart = first(Close),\n            YearEnd = last(Close),\n            PriceChange = (YearEnd - YearStart) / YearStart * 100) %&gt;%\n  ungroup()\n\n\n`summarise()` has grouped output by 'symbol'. You can override using the\n`.groups` argument.\n\n\n\n\nCode\nfiltered_data &lt;- yearly_prices %&gt;%\n  filter(Year &gt;= 2014, Year &lt;= 2022)\nggplot(filtered_data, aes(x = symbol, y = PriceChange, fill = symbol)) +\n  geom_bar(stat = \"identity\") +\n  facet_wrap(~ Year, scales = \"free_x\") +\n  theme_minimal() +\n  labs(title = \"Annual Price Change of Stocks\",\n       x = \"Stock Symbol\",\n       y = \"Price Change (%)\") +\n  coord_flip() + \n  theme(axis.text.x = element_text(angle = 90, hjust = 1), \n        legend.position = \"none\")\n\n\n\n\n\nWe can see that in 2018 and 2022, the movement of prices are very different across stocks. While in other years, the overall directions of the change in price were similar, the directions of change in price of individual stocks varied a lot in 2018 and 2022.\n\n\nCode\nyearly_prices &lt;- combined_next25_df %&gt;%\n  mutate(Year = year(Date)) %&gt;%\n  group_by(symbol, Year) %&gt;%\n  summarize(YearStart = first(Close),\n            YearEnd = last(Close),\n            PriceChange = (YearEnd - YearStart) / YearStart * 100) %&gt;%\n  ungroup()\n\n\n`summarise()` has grouped output by 'symbol'. You can override using the\n`.groups` argument.\n\n\n\n\nCode\nfiltered_data &lt;- yearly_prices %&gt;%\n  filter(Year &gt;= 2014, Year &lt;= 2022)\nggplot(filtered_data, aes(x = symbol, y = PriceChange, fill = symbol)) +\n  geom_bar(stat = \"identity\") + \n  facet_wrap(~ Year, scales = \"free_x\") +\n  theme_minimal() +\n  labs(title = \"Annual Price Change of Stocks\",\n       x = \"Stock Symbol\",\n       y = \"Price Change (%)\") +\n  coord_flip() + \n  theme(axis.text.x = element_text(angle = 90, hjust = 1), \n        legend.position = \"none\")\n\n\n\n\n\nWhen we look at the next batch, in 2015, they also show variations in directions of change of price. One interesting finding is that AMD during many years moved against the overall direction of change of price. For example, during 2014, while most stocks showed increase in prices, AMD had a large drop in price. However, in 2016, while other stocks did not have great change in price, the price of AMD’s stock increased by over 300%."
  },
  {
    "objectID": "results.html#analysis-by-sectors",
    "href": "results.html#analysis-by-sectors",
    "title": "3  Results",
    "section": "3.2 Analysis by sectors",
    "text": "3.2 Analysis by sectors\nSeeing such large variation in stock prices, we would like to further dig in by looking at different sectors.\n\n\nCode\nsectors &lt;- read.csv(\"data/sectors.csv\")\nstock_prices = read.csv(\"data/stocks2year.csv\")\nstock_prices$sector = \"\"\nfor (i in (1: 499)){\n  s = stock_prices[i,]$symbol\n  sec = ifelse(length(sectors[sectors$Symbol==s,])==0,NA,sectors[sectors$Symbol==s,]$Sector)\n  stock_prices[i,]$sector = sec\n}\n\n\n\n\nCode\ng &lt;- stock_prices[stock_prices$symbol!=\"CMG\" & stock_prices$symbol!=\"AZO\",] |&gt; drop_na() |&gt; ggplot(aes(x = close, y=close2, color=sector,text=symbol)) + \n  geom_point(size=1) + \n  stat_function(fun = function(x)  x, color = \"black\") + \n  stat_function(fun = function(x)  0.8 * x, color = \"black\") + \n  stat_function(fun = function(x)  1.2 * x, color = \"black\")\nggplotly(g)\n\n\n\n\n\n\n3 lines are drawn to mark the 20% return, 0% return and -20% return. By double click on individual sectors, We can see that energy companies were doing great for the last 2 years with over 20% return, while telecommunications services were not doing well and a large number of financial companies were losing value.\n\n\nCode\nstock_prices$return &lt;- stock_prices$close2/stock_prices$close - 1\nstock_prices$performance &lt;- cut(stock_prices$return, breaks = c(-10,-0.2,0,0.2,10), labels=c('less than -20%', '-20 to 0 %', '0 to 20 %', 'over 20%'))\nstock_prices|&gt; drop_na() |&gt; ggplot(aes(y = sector)) + geom_bar(fill=\"cornflowerblue\") + facet_wrap(~performance)\n\n\n\n\n\nThe performance of consumer discretionary sector seems to be evenly distributed over four categories. It is rare for consumer staples sector and materials sector companies to have more than 20% growth over 2 years.\n\n\nCode\nAPR21 = read.csv(\"daily-treasury-rates-2021.csv\")\nAPR22 = read.csv(\"daily-treasury-rates-2022.csv\")\nAPR23 = read.csv(\"daily-treasury-rates-2023.csv\")\nAPR = bind_rows(APR21,APR22,APR23)\nhead(APR)\n\n\n        Date X1.Mo X2.Mo X3.Mo X6.Mo X1.Yr X2.Yr X3.Yr X5.Yr X7.Yr X10.Yr\n1 12/31/2021  0.06  0.05  0.06  0.19  0.39  0.73  0.97  1.26  1.44   1.52\n2 12/30/2021  0.06  0.06  0.05  0.19  0.38  0.73  0.98  1.27  1.44   1.52\n3 12/29/2021  0.01  0.02  0.05  0.19  0.38  0.75  0.99  1.29  1.47   1.55\n4 12/28/2021  0.03  0.04  0.06  0.20  0.39  0.74  0.99  1.27  1.41   1.49\n5 12/27/2021  0.04  0.05  0.06  0.21  0.33  0.76  0.98  1.26  1.41   1.48\n6 12/23/2021  0.04  0.05  0.07  0.18  0.31  0.71  0.97  1.25  1.42   1.50\n  X20.Yr X30.Yr X4.Mo\n1   1.94   1.90    NA\n2   1.97   1.93    NA\n3   2.00   1.96    NA\n4   1.94   1.90    NA\n5   1.92   1.88    NA\n6   1.94   1.91    NA\n\n\nCode\nunemployment = read.csv(\"data/unemployment.csv\")\nsp500 = read.csv(\"data/sp500.csv\")\nsp500 &lt;- sp500 |&gt; mutate(Date = as.Date(Date, format = \"%Y/%m/%d\"))\ndf &lt;- APR[c(1,4,6,11)]\ndf &lt;- df |&gt; mutate(Date = as.Date(Date, format = \"%m/%d/%Y\"))\nunemployment &lt;- unemployment |&gt; mutate(date = as.Date(date, format = \"%Y/%m/%d\"))\nunemployment$date = format(unemployment$date, \"%m/%Y\")\ndf$Date = format(df$Date, \"%m/%Y\")\nsp500$Date = format(sp500$Date, \"%m/%Y\")\nsp500$Close = readr::parse_number(sp500$Close)\n\nunemployment$X3.Mo = NA\nunemployment$X1.Yr = NA\nunemployment$X10.Yr = NA\nunemployment$price = NA\ns1 &lt;- df|&gt;group_by(Date)|&gt;summarise(rate=mean(X3.Mo))\ns2 &lt;- df|&gt;group_by(Date)|&gt;summarise(rate=mean(X1.Yr))\ns3 &lt;- df|&gt;group_by(Date)|&gt;summarise(rate=mean(X10.Yr))\ns4 &lt;- sp500|&gt;group_by(Date)|&gt;summarise(rate=mean(Close))\nfor (i in (1: 35)){\n  d = unemployment[i,]$date\n  print(d)\n  unemployment[i,]$X3.Mo = s1[s1$Date==d,]$rate\n  unemployment[i,]$X1.Yr = s2[s2$Date==d,]$rate\n  unemployment[i,]$X10.Yr = s3[s3$Date==d,]$rate\n  unemployment[i,]$price = s4[s4$Date==d,]$rate\n}\n\n\n[1] \"01/2021\"\n[1] \"02/2021\"\n[1] \"03/2021\"\n[1] \"04/2021\"\n[1] \"05/2021\"\n[1] \"06/2021\"\n[1] \"07/2021\"\n[1] \"08/2021\"\n[1] \"09/2021\"\n[1] \"10/2021\"\n[1] \"11/2021\"\n[1] \"12/2021\"\n[1] \"01/2022\"\n[1] \"02/2022\"\n[1] \"03/2022\"\n[1] \"04/2022\"\n[1] \"05/2022\"\n[1] \"06/2022\"\n[1] \"07/2022\"\n[1] \"08/2022\"\n[1] \"09/2022\"\n[1] \"10/2022\"\n[1] \"11/2022\"\n[1] \"12/2022\"\n[1] \"01/2023\"\n[1] \"02/2023\"\n[1] \"03/2023\"\n[1] \"04/2023\"\n[1] \"05/2023\"\n[1] \"06/2023\"\n[1] \"07/2023\"\n[1] \"08/2023\"\n[1] \"09/2023\"\n[1] \"10/2023\"\n[1] \"11/2023\"\n\n\nCode\n#unemployment |&gt; drop_na() |&gt; GGally::ggparcoord(columns=c(6,3,2,5), alpha = 0.3)\nparcoords::parcoords(\n    unemployment[c(3,2,6,5)],\n    rownames = F \n    , brushMode = \"1D-axes\"\n    , reorderable = T\n    , queue = T    \n    )\n\n\n\n\n\n\nWe can see that employment rate and interest rate are negatively correlated; Extreme values (too high or too low) of unemployment rate often occurred with low sp500 index while sp500 peaks at low unemployment rates.\n\n\nCode\nstock_prices = read.csv(\"data/stocksCovid.csv\")\nstock_prices$sector = \"\"\nfor (i in (1: 496)){\n  s = stock_prices[i,]$symbol\n  sec = ifelse(length(sectors[sectors$Symbol==s,])==0,NA,sectors[sectors$Symbol==s,]$Sector)\n  stock_prices[i,]$sector = sec\n}\nstock_prices$resilience = (stock_prices$close3-stock_prices$close2)/(stock_prices$close - stock_prices$close2)\ng &lt;- stock_prices |&gt; drop_na() |&gt; ggplot(aes(x = close, y=close2, color=sector,text=symbol)) + \n  geom_point(size=1) + \n  stat_function(fun = function(x)  x, color = \"black\")\nggplotly(g)\n\n\n\n\n\n\nCode\n#write.csv(stock_prices2, \"stocks2year.csv\", row.names = FALSE)\n\n\nPrice of stocks of all sectors dropped a lot in the first few months of Covid. While a few of them stay stationary(no increase not decrease).\n\n\nCode\nstock_prices[stock_prices$resilience &lt; 2 & stock_prices$resilience &gt; 0,] |&gt; drop_na() |&gt; ggplot(aes(y=resilience)) + geom_histogram() + facet_wrap(~sector)\n\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\nThe utilities sector and energy sector had a very hard time recovering from the pandemic, while the financial sector recovers relatively slowly. The information technology sector appears to be the most resilient from the pandemic."
  },
  {
    "objectID": "results.html#takeaways",
    "href": "results.html#takeaways",
    "title": "3  Results",
    "section": "3.3 Takeaways",
    "text": "3.3 Takeaways\nA significant portion of the S&P 500’s value is concentrated in its top stocks. This concentration illustrates the impact that movements in these top stocks can have on the overall index. The first 50 stocks account for over 50% of the portfolio’s value, highlighting the index’s skewed distribution towards larger companies. There’s a clear distinction in performance patterns between the top and bottom stocks of the S&P 500. The top stocks generally mirror the S&P 500’s overall upward trend, while the bottom stocks exhibit more varied and often divergent patterns. This diversity in performance indicates different market dynamics at play across the spectrum of the S&P 500. Sector-specific analysis reveals differing fortunes over the years. For instance, energy companies showed strong returns recently, while financial and telecommunications services sectors struggled. This indicates the varying impact of economic and market conditions on different sectors. ## Limitations and Lessons learned The variation in yearly price change among individual stocks, particularly in 2018 and 2022, demonstrates the variation within the index. Some stocks, like AMD, show counter-cyclical price movements, suggesting unique factors at play for certain companies or sectors. Factors like global economic conditions, regulatory changes, and technological advancements, which can significantly impact stock prices, are not directly accounted for in this analysis. While we tried our best, interpretation of data, especially when looking at sectors or individual stocks, can be subjective and may require deeper investigation to draw concrete conclusions. Conclusion ## Conclusion In conclusion, this visualiztion of the S&P 500 underscores the complexity and diversity within the index. The influence of top stocks on the overall index movement, the similar movement of top stocks, other distinctive patterns of stock performance, and the sector-specific responses to economic conditions collectively paint a multifaceted picture of the market. This study highlights the possibility and important of a visualization approach to understanding stock market dynamics."
  }
]